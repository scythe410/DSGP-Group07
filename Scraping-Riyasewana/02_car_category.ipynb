{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1adf0086",
      "metadata": {
        "id": "1adf0086"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. SETUP: Install Google Chrome Stable (The one that works!)\n",
        "# ==========================================\n",
        "# We have to reinstall this because Colab resets if you disconnected\n",
        "!wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add -\n",
        "!sh -c 'echo \"deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main\" >> /etc/apt/sources.list.d/google-chrome.list'\n",
        "!apt-get -y update\n",
        "!apt-get install -y google-chrome-stable\n",
        "\n",
        "# Install Python libraries\n",
        "!pip install selenium webdriver-manager pandas\n",
        "\n",
        "# ==========================================\n",
        "# 2. THE OPTIMIZED SCRAPER\n",
        "# ==========================================\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "BASE_URL = \"https://riyasewana.com/search/cars\"\n",
        "START_PAGE = 2\n",
        "END_PAGE = 4   # Set this to 664 to do the whole site\n",
        "CSV_FILENAME = 'riyasewana_2to4.csv'\n",
        "\n",
        "def setup_driver():\n",
        "    \"\"\"Initializes Selenium with IMAGE LOADING DISABLED for speed.\"\"\"\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument('--disable-gpu')\n",
        "\n",
        "    # --- SPEED HACK: DISABLE IMAGES ---\n",
        "    prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
        "    chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "    # Standard User Agent\n",
        "    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
        "\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "def scrape_vehicle_details(url, driver):\n",
        "    \"\"\"Scrapes a single page.\"\"\"\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        # Reduced sleep because we aren't loading images\n",
        "        time.sleep(1.5)\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        vehicle_data = {'URL': url}\n",
        "\n",
        "        # Title\n",
        "        title = soup.find('h1')\n",
        "        vehicle_data['Title'] = title.get_text(strip=True) if title else 'N/A'\n",
        "\n",
        "        # Contact & Price\n",
        "        spans = soup.find_all('span', class_='moreph')\n",
        "        vehicle_data['Contact'] = spans[0].get_text(strip=True) if len(spans) >= 2 else 'N/A'\n",
        "        vehicle_data['Price'] = spans[1].get_text(strip=True) if len(spans) >= 2 else 'N/A'\n",
        "\n",
        "        # Details Table\n",
        "        details_table = soup.find('table', class_='moret')\n",
        "        if details_table:\n",
        "            for row in details_table.find_all('tr'):\n",
        "                cells = row.find_all('td')\n",
        "                if len(cells) == 4:\n",
        "                    vehicle_data[cells[0].get_text(strip=True)] = cells[1].get_text(strip=True)\n",
        "                    vehicle_data[cells[2].get_text(strip=True)] = cells[3].get_text(strip=True)\n",
        "                elif len(cells) == 2:\n",
        "                    vehicle_data[cells[0].get_text(strip=True)] = cells[1].get_text(strip=True)\n",
        "        return vehicle_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error on {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    driver = setup_driver()\n",
        "    all_ad_links = []\n",
        "\n",
        "    try:\n",
        "        # --- PHASE 1: Collect Links ---\n",
        "        print(f\"--- PHASE 1: Collecting links from page {START_PAGE} to {END_PAGE} ---\")\n",
        "        for page_num in range(START_PAGE, END_PAGE + 1):\n",
        "            search_url = f\"{BASE_URL}?page={page_num}\"\n",
        "            print(f\"Scanning Page {page_num}...\", end=\"\\r\")\n",
        "\n",
        "            driver.get(search_url)\n",
        "            time.sleep(2)\n",
        "\n",
        "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "            links_on_page = soup.select('li.item h2.more a')\n",
        "\n",
        "            for link in links_on_page:\n",
        "                href = link.get('href')\n",
        "                if href: all_ad_links.append(href)\n",
        "\n",
        "        print(f\"\\nTotal Ads Found: {len(all_ad_links)}\")\n",
        "\n",
        "        # --- PHASE 2: Scrape & Save Incrementally ---\n",
        "        print(f\"--- PHASE 2: Scraping Details ---\")\n",
        "\n",
        "        scraped_buffer = []\n",
        "\n",
        "        for i, ad_url in enumerate(all_ad_links):\n",
        "            print(f\"Scraping {i+1}/{len(all_ad_links)}: {ad_url}\")\n",
        "            data = scrape_vehicle_details(ad_url, driver)\n",
        "\n",
        "            if data:\n",
        "                scraped_buffer.append(data)\n",
        "\n",
        "            # --- CHECKPOINT: Save every 10 ads ---\n",
        "            if len(scraped_buffer) >= 10:\n",
        "                df_batch = pd.DataFrame(scraped_buffer)\n",
        "\n",
        "                # Check if file exists to determine if we need headers\n",
        "                file_exists = os.path.isfile(CSV_FILENAME)\n",
        "\n",
        "                df_batch.to_csv(CSV_FILENAME, mode='a', header=not file_exists, index=False)\n",
        "                print(f\"  [Saved 10 records to {CSV_FILENAME}]\")\n",
        "                scraped_buffer = [] # Clear buffer\n",
        "\n",
        "        # Save any remaining data in the buffer\n",
        "        if scraped_buffer:\n",
        "            df_batch = pd.DataFrame(scraped_buffer)\n",
        "            file_exists = os.path.isfile(CSV_FILENAME)\n",
        "            df_batch.to_csv(CSV_FILENAME, mode='a', header=not file_exists, index=False)\n",
        "            print(f\"  [Saved final records to {CSV_FILENAME}]\")\n",
        "\n",
        "        print(\"\\n--- DONE! ---\")\n",
        "\n",
        "        # Verify\n",
        "        if os.path.isfile(CSV_FILENAME):\n",
        "            full_df = pd.read_csv(CSV_FILENAME)\n",
        "            print(f\"Total rows in CSV: {len(full_df)}\")\n",
        "            print(full_df.head())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCRITICAL ERROR: {e}\")\n",
        "    finally:\n",
        "        driver.quit()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}