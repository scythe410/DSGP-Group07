{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "FINAL SCRIPT: This script scrapes all vehicle listings from the Riyasewana 'cars' category.\n",
    "It has been corrected to use the proper CSS selector for finding ad links on search pages.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Install and set up necessary libraries\n",
    "!pip install selenium pandas\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-chromedriver\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Set up the Chrome WebDriver for Colab\n",
    "def setup_driver():\n",
    "    \"\"\"Initializes and returns a Selenium WebDriver.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "# Step 3: Function to scrape a single vehicle page (no changes needed here)\n",
    "def scrape_vehicle_details(url, driver):\n",
    "    \"\"\"Scrapes detailed information from a single vehicle ad URL.\"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        vehicle_data = {'URL': url}\n",
    "\n",
    "        # --- Title, Price, Contact, and Details (logic is correct) ---\n",
    "        title_element = soup.find('h1')\n",
    "        vehicle_data['Title'] = title_element.get_text(strip=True) if title_element else 'N/A'\n",
    "\n",
    "        all_spans = soup.find_all('span', class_='moreph')\n",
    "        vehicle_data['Contact'] = all_spans[0].get_text(strip=True) if len(all_spans) >= 2 else 'N/A'\n",
    "        vehicle_data['Price'] = all_spans[1].get_text(strip=True) if len(all_spans) >= 2 else 'N/A'\n",
    "\n",
    "        details_table = soup.find('table', class_='moret')\n",
    "        if details_table:\n",
    "            rows = details_table.find_all('tr')\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) == 4:\n",
    "                    key1, val1, key2, val2 = [c.get_text(strip=True) for c in cells]\n",
    "                    if key1: vehicle_data[key1] = val1\n",
    "                    if key2: vehicle_data[key2] = val2\n",
    "                elif len(cells) == 2:\n",
    "                    key, val = [c.get_text(strip=True) for c in cells]\n",
    "                    if key: vehicle_data[key] = val\n",
    "        return vehicle_data\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Scraping Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://riyasewana.com/search/cars\"\n",
    "    PAGES_TO_SCRAPE = 3  # Startingggggg with 3 pages for testing\n",
    "\n",
    "    all_ad_links = []\n",
    "    driver = setup_driver()\n",
    "\n",
    "    try:\n",
    "        # Part 1: Collect all ad links from the search result pages\n",
    "        print(f\"--- Starting Part 1: Collecting ad links from {PAGES_TO_SCRAPE} pages ---\")\n",
    "        for page_num in range(1, PAGES_TO_SCRAPE + 1):\n",
    "            search_url = f\"{base_url}?page={page_num}\"\n",
    "            print(f\"Fetching links from page {page_num}: {search_url}\")\n",
    "            driver.get(search_url)\n",
    "            time.sleep(4)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # --- KEY CORRECTION ---\n",
    "            # The selector is now 'li.item h2.more a' to match the actual HTML\n",
    "            links_on_page = soup.select('li.item h2.more a')\n",
    "\n",
    "            for link in links_on_page:\n",
    "                href = link.get('href')\n",
    "                if href:\n",
    "                    all_ad_links.append(href) # The href is already a full URL\n",
    "            print(f\"  - Found {len(links_on_page)} links. Total links collected: {len(all_ad_links)}\")\n",
    "\n",
    "        # Part 2: Scrape the details from each collected link\n",
    "        print(f\"\\n--- Starting Part 2: Scraping details for {len(all_ad_links)} ads ---\")\n",
    "        all_vehicles_data = []\n",
    "        for i, ad_url in enumerate(all_ad_links):\n",
    "            print(f\"Scraping ad {i+1}/{len(all_ad_links)}: {ad_url}\")\n",
    "            data = scrape_vehicle_details(ad_url, driver)\n",
    "            if data:\n",
    "                all_vehicles_data.append(data)\n",
    "\n",
    "        # Part 3: Create a DataFrame and save/display the results\n",
    "        print(\"\\n--- Scraping Complete ---\")\n",
    "        if all_vehicles_data:\n",
    "            df = pd.DataFrame(all_vehicles_data)\n",
    "            print(f\"Successfully scraped {len(df)} vehicle ads.\")\n",
    "            print(\"Displaying the first 5 rows of the dataset:\")\n",
    "            display(df.head())\n",
    "\n",
    "            # Optional: Save the data to a CSV file in your Colab environment\n",
    "            df.to_csv('riyasewana_car_listings.csv', index=False)\n",
    "            print(\"\\nData has been saved to 'riyasewana_car_listings.csv'\")\n",
    "        else:\n",
    "            print(\"No data was scraped. Please check for website structure changes or network issues.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"\\nBrowser session closed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
