{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import concurrent.futures\n",
    "import random\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BASE_URL = \"https://riyasewana.com/search/cars\"\n",
    "# careful \n",
    "TOTAL_PAGES_TO_SCRAPE = 2\n",
    "MAX_THREADS = 10  # Don't go too high hehe\n",
    "CSV_FILENAME = 'riyasewana_fast_scrape.csv'\n",
    "\n",
    "# Headers to mimic a real browser (Prevents immediate blocking)\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "}\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"Helper to download a page and return BeautifulSoup object.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(f\"Request error for {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def scrape_ad_details(ad_url):\n",
    "    \"\"\"Scrapes details from a specific ad URL.\"\"\"\n",
    "    soup = get_soup(ad_url)\n",
    "    if not soup:\n",
    "        return None\n",
    "\n",
    "    data = {'URL': ad_url}\n",
    "    \n",
    "    # 1. Title\n",
    "    title = soup.find('h1')\n",
    "    data['Title'] = title.get_text(strip=True) if title else None\n",
    "    \n",
    "    # 2. Contact & Price\n",
    "    spans = soup.find_all('span', class_='moreph')\n",
    "    if len(spans) >= 2:\n",
    "        data['Contact'] = spans[0].get_text(strip=True)\n",
    "        data['Price'] = spans[1].get_text(strip=True)\n",
    "    \n",
    "    # 3. Table Details\n",
    "    table = soup.find('table', class_='moret')\n",
    "    if table:\n",
    "        for row in table.find_all('tr'):\n",
    "            cells = row.find_all('td')\n",
    "            # Handle 4-cell rows\n",
    "            if len(cells) == 4:\n",
    "                data[cells[0].get_text(strip=True)] = cells[1].get_text(strip=True)\n",
    "                data[cells[2].get_text(strip=True)] = cells[3].get_text(strip=True)\n",
    "            # Handle 2-cell rows\n",
    "            elif len(cells) == 2:\n",
    "                data[cells[0].get_text(strip=True)] = cells[1].get_text(strip=True)\n",
    "                \n",
    "    return data\n",
    "\n",
    "def get_links_from_page(page_num):\n",
    "    \"\"\"Gets all ad URLs from a search result page.\"\"\"\n",
    "    url = f\"{BASE_URL}?page={page_num}\"\n",
    "    soup = get_soup(url)\n",
    "    links = []\n",
    "    if soup:\n",
    "        # Correct selector based on your previous success\n",
    "        anchors = soup.select('li.item h2.more a')\n",
    "        for a in anchors:\n",
    "            links.append(a.get('href'))\n",
    "    return links\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    all_links = []\n",
    "    \n",
    "    print(f\"--- PHASE 1: Collecting Links from {TOTAL_PAGES_TO_SCRAPE} Pages ---\")\n",
    "    \n",
    "    # We use ThreadPoolExecutor to fetch search pages in parallel too\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "        # Map page numbers to the function\n",
    "        future_to_page = {executor.submit(get_links_from_page, i): i for i in range(1, TOTAL_PAGES_TO_SCRAPE + 1)}\n",
    "        \n",
    "        for i, future in enumerate(concurrent.futures.as_completed(future_to_page)):\n",
    "            page_links = future.result()\n",
    "            all_links.extend(page_links)\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Processed {i} search pages... (Total links so far: {len(all_links)})\")\n",
    "\n",
    "    print(f\"\\nTotal Links Found: {len(all_links)}\")\n",
    "    print(f\"--- PHASE 2: Scraping Details (Threads: {MAX_THREADS}) ---\")\n",
    "\n",
    "    scraped_data = []\n",
    "    \n",
    "    # Process links in batches so we can save progress incrementally\n",
    "    BATCH_SIZE = 100\n",
    "    \n",
    "    # Break links into chunks\n",
    "    chunks = [all_links[i:i + BATCH_SIZE] for i in range(0, len(all_links), BATCH_SIZE)]\n",
    "\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing Batch {i+1}/{total_chunks}...\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "            # Start scraping ads in this batch\n",
    "            results = list(executor.map(scrape_ad_details, chunk))\n",
    "        \n",
    "        # Filter out Nones (failed scrapes)\n",
    "        valid_results = [r for r in results if r]\n",
    "        scraped_data.extend(valid_results)\n",
    "        \n",
    "        # --- SAVE PROGRESS CHECKPOINT ---\n",
    "        # This is professional: Save every batch so if Colab crashes, you don't lose everything.\n",
    "        df_batch = pd.DataFrame(valid_results)\n",
    "        \n",
    "        # If file doesn't exist, write header. If it does, append without header.\n",
    "        if not os.path.isfile(CSV_FILENAME):\n",
    "            df_batch.to_csv(CSV_FILENAME, index=False)\n",
    "        else:\n",
    "            df_batch.to_csv(CSV_FILENAME, mode='a', header=False, index=False)\n",
    "            \n",
    "        # Sleep briefly to be nice to the server\n",
    "        time.sleep(1) \n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nDONE! Scraped {len(scraped_data)} ads in {duration:.2f} seconds.\")\n",
    "    print(f\"Data saved to {CSV_FILENAME}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
