{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e3c3416e",
      "metadata": {
        "id": "e3c3416e",
        "outputId": "29ace3ee-aa99-4919-d4aa-9d8e83e2c8e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cloudscraper\n",
            "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.12/dist-packages (from cloudscraper) (3.2.5)\n",
            "Requirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from cloudscraper) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from cloudscraper) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.9.2->cloudscraper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.9.2->cloudscraper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.9.2->cloudscraper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.9.2->cloudscraper) (2025.11.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (4.15.0)\n",
            "Downloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4, cloudscraper\n",
            "Successfully installed bs4-0.0.2 cloudscraper-1.2.71\n",
            "--- PHASE 1: Collecting Links from 3 Pages ---\n",
            "  [!] Blocked or Error: Status 403 for https://riyasewana.com/search/cars?page=1\n",
            "  [!] Blocked or Error: Status 403 for https://riyasewana.com/search/cars?page=2\n",
            "  [!] Blocked or Error: Status 403 for https://riyasewana.com/search/cars?page=3\n",
            "\n",
            "Total Links Found: 0\n",
            "\n",
            "[!] Still getting 0 links? The site might require the Selenium approach.\n"
          ]
        }
      ],
      "source": [
        "# 1. Install the library that bypasses bot protection\n",
        "!pip install cloudscraper pandas bs4\n",
        "\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import concurrent.futures\n",
        "import os\n",
        "import random\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "BASE_URL = \"https://riyasewana.com/search/cars\"\n",
        "TOTAL_PAGES_TO_SCRAPE = 3  # Start small (3 pages) to test!\n",
        "MAX_THREADS = 5            # Reduced to 5 to avoid triggering the firewall again\n",
        "CSV_FILENAME = 'riyasewana_cloudscraper.csv'\n",
        "\n",
        "# Initialize the Scraper (This pretends to be a real Chrome browser)\n",
        "scraper = cloudscraper.create_scraper()\n",
        "\n",
        "def get_soup(url):\n",
        "    \"\"\"Helper to download a page using Cloudscraper.\"\"\"\n",
        "    try:\n",
        "        # We use scraper.get() instead of requests.get()\n",
        "        response = scraper.get(url, timeout=15)\n",
        "\n",
        "        # Check if we were blocked\n",
        "        if response.status_code == 200:\n",
        "            return BeautifulSoup(response.content, 'html.parser')\n",
        "        else:\n",
        "            print(f\"  [!] Blocked or Error: Status {response.status_code} for {url}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"  [!] Connection error for {url}: {e}\")\n",
        "    return None\n",
        "\n",
        "def get_links_from_page(page_num):\n",
        "    \"\"\"Gets all ad URLs from a search result page.\"\"\"\n",
        "    url = f\"{BASE_URL}?page={page_num}\"\n",
        "    soup = get_soup(url)\n",
        "    links = []\n",
        "    if soup:\n",
        "        # Selector for the ad links\n",
        "        anchors = soup.select('li.item h2.more a')\n",
        "        for a in anchors:\n",
        "            links.append(a.get('href'))\n",
        "\n",
        "    # Random sleep to look like a human\n",
        "    time.sleep(random.uniform(0.5, 1.5))\n",
        "    return links\n",
        "\n",
        "def scrape_ad_details(ad_url):\n",
        "    \"\"\"Scrapes details from a specific ad URL.\"\"\"\n",
        "    soup = get_soup(ad_url)\n",
        "    if not soup:\n",
        "        return None\n",
        "\n",
        "    data = {'URL': ad_url}\n",
        "\n",
        "    try:\n",
        "        # 1. Title\n",
        "        title = soup.find('h1')\n",
        "        data['Title'] = title.get_text(strip=True) if title else 'N/A'\n",
        "\n",
        "        # 2. Contact & Price\n",
        "        spans = soup.find_all('span', class_='moreph')\n",
        "        if len(spans) >= 2:\n",
        "            data['Contact'] = spans[0].get_text(strip=True)\n",
        "            data['Price'] = spans[1].get_text(strip=True)\n",
        "\n",
        "        # 3. Table Details\n",
        "        table = soup.find('table', class_='moret')\n",
        "        if table:\n",
        "            for row in table.find_all('tr'):\n",
        "                cells = row.find_all('td')\n",
        "                if len(cells) == 4:\n",
        "                    data[cells[0].get_text(strip=True)] = cells[1].get_text(strip=True)\n",
        "                    data[cells[2].get_text(strip=True)] = cells[3].get_text(strip=True)\n",
        "                elif len(cells) == 2:\n",
        "                    data[cells[0].get_text(strip=True)] = cells[1].get_text(strip=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing details for {ad_url}: {e}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    all_links = []\n",
        "\n",
        "    print(f\"--- PHASE 1: Collecting Links from {TOTAL_PAGES_TO_SCRAPE} Pages ---\")\n",
        "\n",
        "    # We do pages strictly sequentially first to ensure we don't get IP banned\n",
        "    for i in range(1, TOTAL_PAGES_TO_SCRAPE + 1):\n",
        "        print(f\"Scanning Page {i}...\", end='\\r')\n",
        "        links = get_links_from_page(i)\n",
        "        all_links.extend(links)\n",
        "        # Sleep slightly to be polite\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    print(f\"\\nTotal Links Found: {len(all_links)}\")\n",
        "\n",
        "    if len(all_links) > 0:\n",
        "        print(f\"--- PHASE 2: Scraping Details (Threads: {MAX_THREADS}) ---\")\n",
        "        scraped_data = []\n",
        "        BATCH_SIZE = 50\n",
        "        chunks = [all_links[i:i + BATCH_SIZE] for i in range(0, len(all_links), BATCH_SIZE)]\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Processing Batch {i+1}/{len(chunks)} ({len(chunk)} ads)...\")\n",
        "\n",
        "            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
        "                results = list(executor.map(scrape_ad_details, chunk))\n",
        "\n",
        "            valid_results = [r for r in results if r]\n",
        "            scraped_data.extend(valid_results)\n",
        "\n",
        "            # Save Backup\n",
        "            df_batch = pd.DataFrame(valid_results)\n",
        "            if not os.path.isfile(CSV_FILENAME):\n",
        "                df_batch.to_csv(CSV_FILENAME, index=False)\n",
        "            else:\n",
        "                df_batch.to_csv(CSV_FILENAME, mode='a', header=False, index=False)\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(f\"\\nDONE! Scraped {len(scraped_data)} ads in {end_time - start_time:.2f} seconds.\")\n",
        "        print(f\"Data saved to {CSV_FILENAME}\")\n",
        "\n",
        "        # Display Preview\n",
        "        df = pd.read_csv(CSV_FILENAME)\n",
        "        print(df.head())\n",
        "    else:\n",
        "        print(\"\\n[!] Still getting 0 links? The site might require the Selenium approach.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}